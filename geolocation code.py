# -*- coding: utf-8 -*-
"""Copy of Untitled38.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I4cNQHW56fsjiD1VoajHuqzL8cOREu_V
"""

from geopy.geocoders import Nominatim

geolocator = Nominatim(user_agent="MyGeocoder/1.0")

zipcode = input("Enter the zipcode: ")
print("\nZipcode:", zipcode)

try:
    location = geolocator.geocode(zipcode)
    if location:
        print("Details of the said pincode:")
        print(location.address)
    else:
        print("Location not found.")
except Exception as e:
    print("Error:", str(e))

from geopy.geocoders import Nominatim

def geo_coder(zipcode):
  geolocator = Nominatim(user_agent="MyGeocoder/1.0")

  try:
      location = geolocator.geocode(zipcode)
      if location:
          print("Details of the said pincode:")
          print(location.address)
      else:
          print("Location not found.")
  except Exception as e:
      print("Error:", str(e))


zipcode = input("Enter the zipcode: ")
print("\nZipcode:", zipcode)
result = geo_coder(zipcode)

"""# **ML CODE**"""

"/content/drive/MyDrive/Zipcodes/allCountriesCSV.csv"

# suppress display of warnings
import warnings
warnings.filterwarnings("ignore")

# 'Pandas' is used for data manipulation and analysis
import pandas as pd

# 'Numpy' is used for mathematical operations on large, multi-dimensional arrays and matrices
import numpy as np

# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

# 'Seaborn' is based on matplotlib; used for plotting statistical graphics
import seaborn as sns

# import 'is_string_dtype' to check if the type of input is string
from pandas.api.types import is_string_dtype

# import various functions to perform classification
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.linear_model import LinearRegression

# read the excel data file
df = pd.read_csv("/content/drive/MyDrive/Zipcodes/allCountriesCSV.csv")

# display the top 5 rows of the dataframe
# df.head()

data = df.drop(["COMMUNITY", "SHORT_COMMUNITY", "SHORT_STATE", "SHORT_COUNTY", "ACCURACY", "STATE", "COUNTY", "COUNTRY", "CITY"], axis=1)

import pandas as pd
import numpy as np
import re

def convert_to_int(value):
    cleaned_value = re.sub(r'\D', '', str(value))  # Remove non-numeric characters
    return pd.to_numeric(cleaned_value, errors='coerce')

data['Converted_POSTAL_CODE'] = data['POSTAL_CODE'].apply(convert_to_int)

# Now, the "Converted_POSTAL_CODE" column contains integers where possible and NaN for non-convertible values
print(data)

# import re
# def convert_to_int_or_keep(value):
#     cleaned_value = re.sub(r'\D', '', str(value))  # Remove non-numeric characters
#     try:
#         return int(cleaned_value)
#     except (ValueError, TypeError):
#         return value

# data['Converted_POSTAL_CODE'] = data['POSTAL_CODE'].apply(convert_to_int_or_keep)

# # Now, the "Converted_POSTAL_CODE" column contains integer values (where possible) and non-integer values
# print(data)

data = data.dropna(subset=['Converted_POSTAL_CODE'])

# Now, the DataFrame contains only rows without NaN values in the "Converted_POSTAL_CODE" column
print(data)

data.isna().sum()

zipcode_to_check = 637020
exists_in_data_cleaned = zipcode_to_check in data['Converted_POSTAL_CODE']

if exists_in_data_cleaned:
    print(f"The value {zipcode_to_check} exists in the data DataFrame.")
else:
    print(f"The value {zipcode_to_check} does not exist in the data_c DataFrame.")

data_cleaned = data

data_cleaned.dtypes

data_cleaned.shape

import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load your dataset (replace 'your_dataset.csv' with the actual dataset filename)
# data = pd.read_csv('your_dataset.csv')

# Preprocess the data (e.g., handle missing values, remove duplicates)

# Split data into training and testing sets
X = data_cleaned[['Converted_POSTAL_CODE']]
y = data_cleaned[['LATITUDE', 'LONGITUDE']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Create an imputer that fills NaN values with the mean
# imputer = SimpleImputer(strategy='mean')

# # Fit the imputer on your X_train data and transform both X_train and X_test
# X_train = imputer.fit_transform(X_train)
# X_test = imputer.transform(X_test)

# Train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# You can now use the trained model to predict geolocations based on zip codes.

X_train

y_train

zip_code_2d = np.array(6025).reshape(1, -1)

print(model.predict(zip_code_2d))

new_prediction = print(model.predict(zip_code_2d))

import duckdb
duckdb.query("SELECT * FROM data_cleaned where Converted_POSTAL_CODE=637020") # returns a result dataframe

"""Using Random forest"""

# import pandas as pd
# from sklearn.model_selection import train_test_split, RandomizedSearchCV
# from sklearn.ensemble import RandomForestRegressor

# # # Sample data
# # data = pd.DataFrame({'Converted_POSTAL_CODE': [1234, 5678, 9012, 3456, 7890],
# #                      'LATITUDE': [37.7749, 34.0522, 40.7128, 41.8781, 33.6846],
# #                      'LONGITUDE': [-122.4194, -118.2437, -74.0060, -87.6298, -117.8265]})

# # Split the data into X and y
# X = data_cleaned[['Converted_POSTAL_CODE']]
# y = data_cleaned[['LATITUDE', 'LONGITUDE']]

# # Split the data into a training set and a testing set
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Define the model
# model = RandomForestRegressor()

# # Hyperparameter grid for RandomizedSearchCV
# param_dist = {
#     'n_estimators': [10, 50, 100],
#     'max_depth': [None, 10, 20],
#     'min_samples_split': [2, 5],
#     'min_samples_leaf': [1, 2]
# }

# # Create RandomizedSearchCV object with a limited number of iterations and fewer cross-validation folds
# random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=5, scoring='neg_mean_squared_error', cv=3)

# # Fit the model with RandomizedSearchCV
# random_search.fit(X_train, y_train)

# # Get the best hyperparameters
# best_params = random_search.best_params_
# print("Best Hyperparameters:", best_params)

# # Evaluate the model on the test set
# best_model = random_search.best_estimator_
# y_pred = best_model.predict(X_test)

# # Calculate evaluation metrics (you can use other metrics)
# from sklearn.metrics import mean_squared_error
# mse = mean_squared_error(y_test, y_pred)
# print("Mean Squared Error:", mse)

# import pandas as pd
# import numpy as np
# from sklearn.model_selection import GridSearchCV, train_test_split
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.metrics import mean_squared_error

# # Sample data
# # data = pd.DataFrame({
# #     'Converted_POSTAL_CODE': [1001, 2002, 3003, 4004, 5005],
# #     'LATITUDE': [42.3601, 34.0522, 40.7128, 29.7604, 39.9042],
# #     'LONGITUDE': [-71.0589, -118.2437, -74.0060, -95.3698, -75.1652]
# # })

# X = data_cleaned[['Converted_POSTAL_CODE']]
# y = data_cleaned[['LATITUDE', 'LONGITUDE']]

# # Split data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Define the regression model (Random Forest in this example)
# model = RandomForestRegressor()

# # Define hyperparameter grid for Grid Search
# param_grid = {
#     'n_estimators': [10, 50, 100, 200],
#     'max_depth': [None, 10, 20, 30],
#     'min_samples_split': [2, 5, 10]
# }

# # Create Grid Search with cross-validation
# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# # Fit the grid search to the data
# grid_search.fit(X_train, y_train)

# # Get the best hyperparameters
# best_params = grid_search.best_params_

# # Train the model with the best hyperparameters
# best_model = RandomForestRegressor(**best_params)
# best_model.fit(X_train, y_train)

# # Make predictions on the test set
# y_pred = best_model.predict(X_test)

# # Evaluate the model using Mean Squared Error (MSE)
# mse = mean_squared_error(y_test, y_pred)
# print("Best Hyperparameters:", best_params)
# print("Mean Squared Error:", mse)

"""# **Using Tensorflow**"""

import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load your dataset


# Prepare the data
X = data[['Converted_POSTAL_CODE']]
y = data[['LATITUDE', 'LONGITUDE']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Data preprocessing (standardization)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define a neural network model
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(2)  # Two output neurons for latitude and longitude
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model
loss = model.evaluate(X_test, y_test)
print("Test Loss:", loss)

# Make predictions
predictions = model.predict(X_test)

# Optionally, you can convert the predictions back to original scale if you standardized the data earlier.
# predictions = scaler.inverse_transform(predictions)

# Now you can use the model for geolocation predictions using zip codes.

zip_code_2d = np.array(6025).reshape(1, -1)

print(model.predict(zip_code_2d))

import pickle

# Your trained model (replace with your model object)
model = model

# Specify the file path where you want to save the model
model_filename = "model.pkl"

# Save the model to the file
with open(model_filename, 'wb') as model_file:
    pickle.dump(model, model_file)